# StormSeg: Segmenting Storm-Damaged Trees From Point Cloud Data

## Abstract

Storm-induced forest damage creates major challenges for recovery and management, especially where large-scale assessment tools are lacking. Manual field surveys are slow, labor-intensive, and often inconsistent. This project presents a modular pipeline for segmenting and classifying storm-damaged trees from airborne laser scanning (ALS) point cloud data. Building on top of FSCT, we combine semantic segmentation with additional processing steps such as resolution enhancement and height-based filtering to separate upright trees from fallen debris. Visual evaluation suggests the pipeline can capture key structural features of storm-affected forests, offering a foundation for automated post-disaster analysis.

For Detailed explanation please refer [report](./Report.pdf).

## Installation

1. Clone the repository
2. Create a virtual environment (recommended):
```bash
conda create --name YOUR_ENVIRONMENT_NAME_HERE python==3.9
conda activate YOUR_ENVIRONMENT_NAME_HERE
conda install pip
```

3. Install requirements:
```bash
pip install -r requirements.txt
```

The installation has been tested on Windows 10 and Linux systems. For users without an NVIDIA GPU, set `use_CPU_only` to True in the configuration parameters.

## How to Use

### Additional Scripts

1. augment.py - This script can be independently used to increase the resolution of the input point cloud 
2. post-segmentation.py - This script is run to handle all post-processing after semantic segmentation using two files generated, i.e. segmented.las and DTM.las

### Semantic Segmenation Pipeline

1. Navigate to the `scripts` directory (You have to be in this directory or the code won't execute correctly)
2. Open and configure the `main_config.py` file in the `configs` folder
3. Run the main script:
```bash
python run.py
```

4. A file dialog will appear, allowing you to select one or more `.las` or `.laz` files for processing

### Configuration Parameters

Key parameters to configure in `configs/main_config.py`:

- **Hardware Settings**:
  - `batch_size` - Number of batches for processing. Adjust based on available GPU memory. Must be â‰¥ 2.
  - `num_cpu_cores` - Number of CPU cores to use (0 = all cores).
  - `use_CPU_only` - Set to True if no NVIDIA GPU is available.
- **Processing Parameters**:
  - `slice_thickness` - Thickness of horizontal slices for stem detection (default: 0.15m).
  - `slice_increment` - Vertical increment between slices (default: 0.05m).

  
### Processing Workflow

FSCT process pipeline point clouds in three main steps:

1. **Preprocessing** (`preprocessing.py`): Prepares the point cloud by subsampling and dividing it into boxes for segmentation
2. **Segmentation** (`inference.py`): Uses a deep learning model to classify points into terrain, vegetation, CWD, and stem classes
3. **Postprocessing** (`post_segmentation_script.py`): Creates the DTM and refines the segmented point cloud

## Outputs

FSCT generates output files in a directory named `<point_cloud_filename>_FSCT_output/` including:

### Main Outputs
- `DTM.las` - Digital Terrain Model
- `segmented.las` - Semantically segmented point cloud
- `augmented.las` - Augemented point cloud file generated by augment.py
- `stem_only.las` - Semantically segmented point cloud with upright trees generated by post-segmentation.py
- `filtered_non_stem.las` - Cleaned segmented point cloud with fallen trees generated by post-segmentation.py

## Reference
 FSCT Github: https://github.com/SKrisanski/FSCT
